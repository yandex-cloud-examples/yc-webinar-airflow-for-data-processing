В данном репозитории опубликованы материалы для семинара Обработка данных на Apache Airflow в Yandex Cloud (https://www.youtube.com/watch?v=jF3YemOVofQ).
Общая архитектура стенда и описание процессов обработки данных изложены в видео.

Ниже представлено два Airflow DAG файла:
1. DataProcProcessing.py - демонстрация обработки данных на временном Data Proc кластере.
Для его работы необходимо создать следующие переменные Airflow 
    - DP_SA_AUTH_JSON_PATH - JSON credential файл для вызовов сервиса YC DataProc
    - DP_PUBLIC_SSH_KEY - Публичная часть SSH ключа, необходимая для создания узлов DataProc кластера
    - S3_KEY_ID - Ключ для доступа к используемым бакетам YC S3
    - S3_SECRET_KEY - секретная часть ключа
Эти параметры необходимы для программного создания используемых Connection. Тем не менее, Connections можно создать вручную через admin интерфейс Airflow (с соответствующими conn_id). В этом случае Variables можно не создавать.  
Так же в тексте самого DAG в переменные вынесены ID, имена и URL связанных ресурсов. Их необходимо изменить на свои.
Кроме того, в качестве внешнего Metastore кластера в данном примере используется дополнительный DataProc кластер, который должен быть создан заранее.
Для выполнения pyspark задания используется скрипт DataProcessing.py, который необходимо разместить в отдельном бакете S3.

2. ETLOrchestration.py - демонстрация орекстрации ETL процесса, выполняемого в БД Greenplum.
Переменные Airflow, которые нужно создать предварительно
    - DEMO_USER - пользователь БД Postgres, под которым будет осуществляться подключение
    - DEMO_PWD - пароль этого пользователя
    - DEMO_KEY - статический ключ доступа с помощью которого будет осуществляться обращение к S3 из Clickhouse
    - DEMO_SECRET - секретная часть этого ключа
Эти параметры необходимы для программного создания используемых Connection. Тем не менее, Connections можно создать вручную через admin интерфейс Airflow (с соответствующими conn_id). В этом случае Variables DEMO_USER и DEMO_PWD можно не создавать.
Для этого сценария необходимо предварительно создать:
    - Greenplum кластер, в котором необходимо создать хранимую процедуру реализующую ETL, а так же PXF таблицы для обращения к S3 и Postgres БД (файлы etl_function.sql и pxf_tables.sql).
    - Clickhouse кластер, который используется вторым оператором DAG, для построения витрин.
